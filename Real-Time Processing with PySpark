from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, FloatType

spark = SparkSession.builder.appName('RealTimeProcessing').getOrCreate()

# Define the schema
schema = StructType([
    StructField("sensor_id", StringType(), True),
    StructField("value", FloatType(), True),
    StructField("timestamp", StringType(), True)
])

# Read from Kafka
df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "localhost:9092").option("subscribe", "sensor_data").load()

# Parse the JSON data
df = df.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*")

# Write the streaming data to SQLite
def write_to_sqlite(df, epoch_id):
    sqlite_url = "jdbc:sqlite:realtime_data.db"
    df.write.jdbc(url=sqlite_url, table="sensor_data", mode="append")

df.writeStream.foreachBatch(write_to_sqlite).start().awaitTermination()
